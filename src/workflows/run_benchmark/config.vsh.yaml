name: "run_benchmark"
namespace: "workflows"
argument_groups:
  - name: Inputs
    arguments:
      - name: "--input_train"
        __merge__: "/src/api/file_train.yaml"
        required: true
        direction: input
      - name: "--input_test"
        __merge__: "/src/api/file_test.yaml"
        required: true
        direction: input
  - name: Outputs
    arguments:
      - name: "--output_scores"
        type: file
        required: true
        direction: output
        description: A yaml file containing the scores of each of the methods
        default: score_uns.yaml
      - name: "--output_method_configs"
        type: file
        required: true
        direction: output
        default: method_configs.yaml
      - name: "--output_metric_configs"
        type: file
        required: true
        direction: output
        default: metric_configs.yaml
      - name: "--output_dataset_info"
        type: file
        required: true
        direction: output
        default: dataset_uns.yaml
      - name: "--output_task_info"
        type: file
        required: true
        direction: output
        default: task_info.yaml
  - name: Methods
    arguments:
      - name: "--method_ids"
        type: string
        multiple: true
        description: A list of method ids to run. If not specified, all methods will be run.
resources:
  - type: nextflow_script
    path: main.nf
    entrypoint: run_wf
  - type: file
    path: /_viash.yaml
dependencies:
  - name: common/check_dataset_schema
    repository: openproblems
  - name: common/extract_metadata
    repository: openproblems
  - name: control_methods/no_denoising
  - name: control_methods/perfect_denoising
  - name: methods/alra
  - name: methods/dca
  - name: methods/knn_smoothing
  - name: methods/magic
  - name: metrics/mse
  - name: metrics/poisson
runners:
  - type: nextflow
